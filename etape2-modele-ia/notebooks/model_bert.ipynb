{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55178936",
   "metadata": {},
   "source": [
    "# üöÄ Mod√®le Avanc√© - BERT Fine-tuning\n",
    "\n",
    "Ce notebook entra√Æne un mod√®le BERT fine-tun√© pour la d√©tection de toxicit√©.\n",
    "\n",
    "## Objectifs\n",
    "- ü§ñ Fine-tuner BERT pr√©-entra√Æn√©\n",
    "- üìà Am√©liorer le F1-Score (objectif > 0.75)\n",
    "- ‚ö° Optimiser pour l'inf√©rence\n",
    "- üîÑ Comparer avec le mod√®le simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bd07d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TP_deploiement_1\\digital-social-score\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Imports termin√©s !\n",
      "üñ•Ô∏è Device: cpu\n",
      "üïê D√©marrage: 13:18:56\n",
      "‚ö†Ô∏è CPU seulement - L'entra√Ænement sera plus lent\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Transformers et tokenizers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Scikit-learn pour m√©triques\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# V√©rifier CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üì¶ Imports termin√©s !\")\n",
    "print(f\"üñ•Ô∏è Device: {device}\")\n",
    "print(f\"üïê D√©marrage: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üöÄ GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CPU seulement - L'entra√Ænement sera plus lent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd78c42",
   "metadata": {},
   "source": [
    "## 1. üìÅ Chargement des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09adcace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Chargement des donn√©es pr√©process√©es...\n",
      "‚úÖ Train: 20,000 lignes\n",
      "‚úÖ Test: 20,000 lignes\n",
      "\n",
      "üìä Split BERT train/validation:\n",
      "  Train: 17,000 textes\n",
      "  Validation: 3,000 textes\n",
      "  Train toxic: 1,751 (10.3%)\n",
      "  Val toxic: 309 (10.3%)\n",
      "\n",
      "üî¨ MODE TEST: √âchantillonnage √† 5000 exemples...\n",
      "  Train √©chantillonn√©: 5,000\n",
      "  Validation √©chantillonn√©: 1,000\n",
      "‚úÖ Train: 20,000 lignes\n",
      "‚úÖ Test: 20,000 lignes\n",
      "\n",
      "üìä Split BERT train/validation:\n",
      "  Train: 17,000 textes\n",
      "  Validation: 3,000 textes\n",
      "  Train toxic: 1,751 (10.3%)\n",
      "  Val toxic: 309 (10.3%)\n",
      "\n",
      "üî¨ MODE TEST: √âchantillonnage √† 5000 exemples...\n",
      "  Train √©chantillonn√©: 5,000\n",
      "  Validation √©chantillonn√©: 1,000\n"
     ]
    }
   ],
   "source": [
    "# Chargement des donn√©es pr√©process√©es\n",
    "print(\"üìä Chargement des donn√©es pr√©process√©es...\")\n",
    "\n",
    "train_df = pd.read_csv('../data/train_preprocessed.csv')\n",
    "test_df = pd.read_csv('../data/test_preprocessed.csv')\n",
    "\n",
    "print(f\"‚úÖ Train: {train_df.shape[0]:,} lignes\")\n",
    "print(f\"‚úÖ Test: {test_df.shape[0]:,} lignes\")\n",
    "\n",
    "# Pr√©paration des donn√©es pour BERT (texte l√©ger)\n",
    "X_train_text = train_df['comment_bert'].fillna('').astype(str)\n",
    "y_train = train_df['is_toxic'].fillna(0).astype(int)\n",
    "\n",
    "# Split train/validation pour BERT (plus petit validation set pour √©conomiser temps)\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_text, y_train, \n",
    "    test_size=0.15,  # 15% pour validation (vs 20% avant)\n",
    "    random_state=42, \n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Split BERT train/validation:\")\n",
    "print(f\"  Train: {len(X_train_split):,} textes\")\n",
    "print(f\"  Validation: {len(X_val_split):,} textes\")\n",
    "print(f\"  Train toxic: {y_train_split.sum():,} ({y_train_split.sum()/len(y_train_split)*100:.1f}%)\")\n",
    "print(f\"  Val toxic: {y_val_split.sum():,} ({y_val_split.sum()/len(y_val_split)*100:.1f}%)\")\n",
    "\n",
    "# √âchantillonner pour test rapide (optionnel)\n",
    "SAMPLE_SIZE = 5000  # R√©duire pour test rapide, mettre None pour tout\n",
    "if SAMPLE_SIZE and len(X_train_split) > SAMPLE_SIZE:\n",
    "    print(f\"\\nüî¨ MODE TEST: √âchantillonnage √† {SAMPLE_SIZE} exemples...\")\n",
    "    \n",
    "    # Stratified sampling\n",
    "    X_train_sample, _, y_train_sample, _ = train_test_split(\n",
    "        X_train_split, y_train_split,\n",
    "        train_size=SAMPLE_SIZE,\n",
    "        random_state=42,\n",
    "        stratify=y_train_split\n",
    "    )\n",
    "    \n",
    "    X_val_sample, _, y_val_sample, _ = train_test_split(\n",
    "        X_val_split, y_val_split,\n",
    "        train_size=min(1000, len(X_val_split)),  # Max 1000 pour validation\n",
    "        random_state=42,\n",
    "        stratify=y_val_split\n",
    "    )\n",
    "    \n",
    "    X_train_split, y_train_split = X_train_sample, y_train_sample\n",
    "    X_val_split, y_val_split = X_val_sample, y_val_sample\n",
    "    \n",
    "    print(f\"  Train √©chantillonn√©: {len(X_train_split):,}\")\n",
    "    print(f\"  Validation √©chantillonn√©: {len(X_val_split):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577ff1f",
   "metadata": {},
   "source": [
    "## 2. ü§ñ Configuration du Mod√®le BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19d5f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Chargement du mod√®le: distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le charg√© sur cpu\n",
      "üìè Longueur max tokens: 512\n",
      "üî¢ Nombre de param√®tres: 66,955,010\n"
     ]
    }
   ],
   "source": [
    "# Choisir le mod√®le BERT (distilbert pour √™tre plus rapide)\n",
    "MODEL_NAME = \"distilbert-base-uncased\"  # Plus l√©ger et rapide que bert-base-uncased\n",
    "\n",
    "print(f\"ü§ñ Chargement du mod√®le: {MODEL_NAME}\")\n",
    "\n",
    "# Charger tokenizer et mod√®le\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,  # Binaire: toxic/non-toxic\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# D√©placer vers GPU si disponible\n",
    "model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le charg√© sur {device}\")\n",
    "print(f\"üìè Longueur max tokens: {tokenizer.model_max_length}\")\n",
    "print(f\"üî¢ Nombre de param√®tres: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5fd5c",
   "metadata": {},
   "source": [
    "## 3. üî§ Tokenisation des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa0cf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî§ Tokenisation des donn√©es (max_length=128)...\n",
      "  Tokenisation train...\n",
      "  Tokenisation validation...\n",
      "‚úÖ Tokenisation termin√©e\n",
      "üìè Shape train tokens: torch.Size([5000, 128])\n",
      "üìè Shape val tokens: torch.Size([1000, 128])\n",
      "  Tokenisation validation...\n",
      "‚úÖ Tokenisation termin√©e\n",
      "üìè Shape train tokens: torch.Size([5000, 128])\n",
      "üìè Shape val tokens: torch.Size([1000, 128])\n",
      "\n",
      "üìä Longueurs des s√©quences tokenis√©es:\n",
      "  Moyenne: 63.9 tokens\n",
      "  M√©diane: 53.0 tokens\n",
      "  Max: 128 tokens\n",
      "  % tronqu√©s: 19.9%\n",
      "\n",
      "üìä Longueurs des s√©quences tokenis√©es:\n",
      "  Moyenne: 63.9 tokens\n",
      "  M√©diane: 53.0 tokens\n",
      "  Max: 128 tokens\n",
      "  % tronqu√©s: 19.9%\n"
     ]
    }
   ],
   "source": [
    "# Configuration de tokenisation\n",
    "MAX_LENGTH = 128  # R√©duire pour √™tre plus rapide (vs 512 par d√©faut)\n",
    "\n",
    "def tokenize_function(texts):\n",
    "    \"\"\"Tokenise les textes pour BERT\"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "print(f\"üî§ Tokenisation des donn√©es (max_length={MAX_LENGTH})...\")\n",
    "\n",
    "# Tokeniser train\n",
    "print(\"  Tokenisation train...\")\n",
    "train_encodings = tokenize_function(X_train_split.tolist())\n",
    "\n",
    "# Tokeniser validation\n",
    "print(\"  Tokenisation validation...\")\n",
    "val_encodings = tokenize_function(X_val_split.tolist())\n",
    "\n",
    "print(f\"‚úÖ Tokenisation termin√©e\")\n",
    "print(f\"üìè Shape train tokens: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"üìè Shape val tokens: {val_encodings['input_ids'].shape}\")\n",
    "\n",
    "# Analyser la distribution des longueurs\n",
    "train_lengths = (train_encodings['attention_mask'].sum(dim=1)).numpy()\n",
    "print(f\"\\nüìä Longueurs des s√©quences tokenis√©es:\")\n",
    "print(f\"  Moyenne: {train_lengths.mean():.1f} tokens\")\n",
    "print(f\"  M√©diane: {np.median(train_lengths):.1f} tokens\")\n",
    "print(f\"  Max: {train_lengths.max()} tokens\")\n",
    "print(f\"  % tronqu√©s: {(train_lengths == MAX_LENGTH).mean() * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee5a0d",
   "metadata": {},
   "source": [
    "## 4. üìö Cr√©ation des Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ce3dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Datasets cr√©√©s:\n",
      "  Train dataset: 5,000 exemples\n",
      "  Val dataset: 1,000 exemples\n",
      "\n",
      "üîç Test √©chantillon:\n",
      "  Input shape: torch.Size([128])\n",
      "  Label: 0\n",
      "  Attention mask shape: torch.Size([128])\n",
      "  Label: 0\n",
      "  Attention mask shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er les datasets HuggingFace\n",
    "class ToxicityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx] if hasattr(self.labels, 'iloc') else self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Cr√©er les datasets\n",
    "train_dataset = ToxicityDataset(train_encodings, y_train_split)\n",
    "val_dataset = ToxicityDataset(val_encodings, y_val_split)\n",
    "\n",
    "print(f\"üìö Datasets cr√©√©s:\")\n",
    "print(f\"  Train dataset: {len(train_dataset):,} exemples\")\n",
    "print(f\"  Val dataset: {len(val_dataset):,} exemples\")\n",
    "\n",
    "# Test d'un √©chantillon\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nüîç Test √©chantillon:\")\n",
    "print(f\"  Input shape: {sample['input_ids'].shape}\")\n",
    "print(f\"  Label: {sample['labels'].item()}\")\n",
    "print(f\"  Attention mask shape: {sample['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd658bb",
   "metadata": {},
   "source": [
    "## 5. ‚öôÔ∏è Configuration de l'Entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe594c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration d'entra√Ænement:\n",
      "  Epochs: 2\n",
      "  Batch size train: 16\n",
      "  Batch size eval: 32\n",
      "  Warmup steps: 100\n",
      "  Output dir: ../models/bert_model\n",
      "\n",
      "‚è±Ô∏è Estimation:\n",
      "  Steps par epoch: 312\n",
      "  Total steps: 624\n",
      "  Temps estim√©: 52min 0s\n"
     ]
    }
   ],
   "source": [
    "# D√©finir les m√©triques d'√©valuation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Configuration de l'entra√Ænement (optimis√©e pour rapidit√©)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/bert_model',\n",
    "    num_train_epochs=2,              # R√©duire pour test rapide (3 normalement)\n",
    "    per_device_train_batch_size=16,  # Ajuster selon GPU\n",
    "    per_device_eval_batch_size=32,   # Plus grand pour √©valuation\n",
    "    warmup_steps=100,                # R√©duire\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='../models/bert_model/logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,                  # √âvaluer moins souvent\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=400,                  # Multiple de eval_steps\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    dataloader_num_workers=0,        # 0 pour √©viter probl√®mes multiprocessing\n",
    "    remove_unused_columns=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=None                   # Pas de logging externe\n",
    ")\n",
    "\n",
    "print(f\"‚öôÔ∏è Configuration d'entra√Ænement:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size train: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Batch size eval: {training_args.per_device_eval_batch_size}\")\n",
    "print(f\"  Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"  Output dir: {training_args.output_dir}\")\n",
    "\n",
    "# Estimer le temps d'entra√Ænement\n",
    "steps_per_epoch = len(train_dataset) // training_args.per_device_train_batch_size\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "estimated_time = total_steps * (2 if device.type == 'cuda' else 5)  # secondes par step\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Estimation:\")\n",
    "print(f\"  Steps par epoch: {steps_per_epoch}\")\n",
    "print(f\"  Total steps: {total_steps}\")\n",
    "print(f\"  Temps estim√©: {estimated_time//60:.0f}min {estimated_time%60:.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb6577",
   "metadata": {},
   "source": [
    "## 6. üöÄ Entra√Ænement du Mod√®le BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efa8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√âBUT DE L'ENTRA√éNEMENT BERT\n",
      "üìÖ 13:20:16\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 17/626 06:03 < 4:06:03, 0.04 it/s, Epoch 0.05/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cr√©er le trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "print(f\"üöÄ D√âBUT DE L'ENTRA√éNEMENT BERT\")\n",
    "print(f\"üìÖ {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Entra√Æner le mod√®le\n",
    "start_time = time.time()\n",
    "training_results = trainer.train()\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ ENTRA√éNEMENT TERMIN√â !\")\n",
    "print(f\"‚è±Ô∏è Temps total: {training_time//60:.0f}min {training_time%60:.0f}s\")\n",
    "print(f\"üèÅ Loss finale: {training_results.training_loss:.4f}\")\n",
    "\n",
    "# Sauvegarder le mod√®le\n",
    "print(f\"\\nüíæ Sauvegarde du mod√®le...\")\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained('../models/bert_model')\n",
    "print(f\"‚úÖ Mod√®le sauvegard√© dans ../models/bert_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217034b1",
   "metadata": {},
   "source": [
    "## 7. üìä √âvaluation D√©taill√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation sur le set de validation\n",
    "print(f\"üìä √âVALUATION D√âTAILL√âE DU MOD√àLE BERT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Pr√©dictions\n",
    "print(\"üîÆ G√©n√©ration des pr√©dictions...\")\n",
    "start_time = time.time()\n",
    "predictions = trainer.predict(val_dataset)\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "# Extraire les pr√©dictions\n",
    "y_pred_logits = predictions.predictions\n",
    "y_pred_proba = torch.softmax(torch.from_numpy(y_pred_logits), dim=1).numpy()[:, 1]\n",
    "y_pred = np.argmax(y_pred_logits, axis=1)\n",
    "y_true = y_val_split.values if hasattr(y_val_split, 'values') else y_val_split\n",
    "\n",
    "# Calculer les m√©triques\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "auc_roc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "print(f\"\\nüìà R√âSULTATS BERT:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  F1-Score: {f1:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  AUC-ROC: {auc_roc:.4f}\")\n",
    "\n",
    "print(f\"\\n‚ö° PERFORMANCE:\")\n",
    "print(f\"  Temps inf√©rence total: {inference_time:.2f}s\")\n",
    "print(f\"  Temps par texte: {inference_time/len(y_true)*1000:.2f}ms\")\n",
    "print(f\"  Crit√®re F1 > 0.75: {'‚úÖ' if f1 > 0.75 else '‚ùå'}\")\n",
    "print(f\"  Crit√®re temps < 500ms: {'‚úÖ' if (inference_time/len(y_true)*1000) < 500 else '‚ùå'}\")\n",
    "#hhgg\n",
    "# Rapport de classification d√©taill√©\n",
    "print(f\"\\nüìã Rapport de Classification:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Non-Toxic', 'Toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21695e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion et visualisations\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Matrice de confusion\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Non-Toxic', 'Toxic'],\n",
    "            yticklabels=['Non-Toxic', 'Toxic'],\n",
    "            ax=axes[0,0])\n",
    "axes[0,0].set_title('Matrice de Confusion - BERT')\n",
    "axes[0,0].set_ylabel('Actual')\n",
    "axes[0,0].set_xlabel('Predicted')\n",
    "\n",
    "# 2. Distribution des probabilit√©s\n",
    "toxic_probs = y_pred_proba[y_true == 1]\n",
    "non_toxic_probs = y_pred_proba[y_true == 0]\n",
    "\n",
    "axes[0,1].hist(non_toxic_probs, bins=30, alpha=0.7, label='Non-Toxic', color='green', density=True)\n",
    "axes[0,1].hist(toxic_probs, bins=30, alpha=0.7, label='Toxic', color='red', density=True)\n",
    "axes[0,1].set_xlabel('Probabilit√© de Toxicit√©')\n",
    "axes[0,1].set_ylabel('Densit√©')\n",
    "axes[0,1].set_title('Distribution des Probabilit√©s - BERT')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Courbe ROC\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "\n",
    "axes[1,0].plot(fpr, tpr, color='blue', lw=2, label=f'BERT (AUC = {auc_roc:.3f})')\n",
    "axes[1,0].plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random')\n",
    "axes[1,0].set_xlabel('False Positive Rate')\n",
    "axes[1,0].set_ylabel('True Positive Rate')\n",
    "axes[1,0].set_title('Courbe ROC - BERT')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. √âvolution de la loss (si disponible)\n",
    "if hasattr(trainer.state, 'log_history'):\n",
    "    log_history = trainer.state.log_history\n",
    "    train_losses = [log['train_loss'] for log in log_history if 'train_loss' in log]\n",
    "    eval_losses = [log['eval_loss'] for log in log_history if 'eval_loss' in log]\n",
    "    \n",
    "    if train_losses:\n",
    "        axes[1,1].plot(train_losses, label='Train Loss', color='blue')\n",
    "    if eval_losses:\n",
    "        eval_steps = [log['step'] for log in log_history if 'eval_loss' in log]\n",
    "        axes[1,1].plot(eval_steps, eval_losses, label='Eval Loss', color='red', marker='o')\n",
    "    \n",
    "    axes[1,1].set_xlabel('Steps')\n",
    "    axes[1,1].set_ylabel('Loss')\n",
    "    axes[1,1].set_title('√âvolution de la Loss')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'Historique des losses\\nnon disponible', \n",
    "                   ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "    axes[1,1].set_title('√âvolution de la Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf576f",
   "metadata": {},
   "source": [
    "## 8. üîÑ Comparaison avec le Mod√®le Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les r√©sultats du mod√®le simple\n",
    "try:\n",
    "    with open('../models/simple_model/metadata.json', 'r') as f:\n",
    "        simple_metadata = json.load(f)\n",
    "    \n",
    "    print(\"üîÑ COMPARAISON MOD√àLE SIMPLE vs BERT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Cr√©er tableau comparatif\n",
    "    comparison_data = {\n",
    "        'M√©trique': ['Accuracy', 'F1-Score', 'AUC-ROC', 'Temps Entra√Ænement', 'Temps Inf√©rence (ms/texte)'],\n",
    "        'Mod√®le Simple (TF-IDF + LR)': [\n",
    "            f\"{simple_metadata['accuracy']:.4f}\",\n",
    "            f\"{simple_metadata['f1_score']:.4f}\",\n",
    "            f\"{simple_metadata['auc_roc']:.4f}\",\n",
    "            f\"{simple_metadata['train_time']:.2f}s\",\n",
    "            f\"{simple_metadata['inference_time_per_text_ms']:.2f}ms\"\n",
    "        ],\n",
    "        'BERT (DistilBERT)': [\n",
    "            f\"{accuracy:.4f}\",\n",
    "            f\"{f1:.4f}\",\n",
    "            f\"{auc_roc:.4f}\",\n",
    "            f\"{training_time:.0f}s\",\n",
    "            f\"{inference_time/len(y_true)*1000:.2f}ms\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Am√©liorations\n",
    "    f1_improvement = f1 - simple_metadata['f1_score']\n",
    "    accuracy_improvement = accuracy - simple_metadata['accuracy']\n",
    "    \n",
    "    print(f\"\\nüìà AM√âLIORATIONS BERT:\")\n",
    "    print(f\"  F1-Score: {f1_improvement:+.4f} ({f1_improvement/simple_metadata['f1_score']*100:+.1f}%)\")\n",
    "    print(f\"  Accuracy: {accuracy_improvement:+.4f} ({accuracy_improvement/simple_metadata['accuracy']*100:+.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüèÜ MEILLEUR MOD√àLE:\")\n",
    "    if f1 > simple_metadata['f1_score']:\n",
    "        print(f\"  BERT wins! (F1: {f1:.4f} vs {simple_metadata['f1_score']:.4f})\")\n",
    "    else:\n",
    "        print(f\"  Simple model wins! (F1: {simple_metadata['f1_score']:.4f} vs {f1:.4f})\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è M√©tadonn√©es du mod√®le simple non trouv√©es\")\n",
    "    comparison_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de36bdc3",
   "metadata": {},
   "source": [
    "## 9. üß™ Tests de Pr√©diction BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de pr√©diction pour BERT\n",
    "def predict_toxicity_bert(text, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"Pr√©dit la toxicit√© avec BERT\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokeniser\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length\n",
    "    ).to(device)\n",
    "    \n",
    "    # Pr√©diction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "        confidence = probabilities[0, prediction].item()\n",
    "        toxic_prob = probabilities[0, 1].item()\n",
    "    \n",
    "    return prediction, toxic_prob\n",
    "\n",
    "# Tests sur des exemples\n",
    "test_texts = [\n",
    "    \"This is a great article, thank you for sharing!\",\n",
    "    \"You are stupid and I hate you\",\n",
    "    \"I disagree with your opinion but respect your right to have it\",\n",
    "    \"This movie is terrible and boring\",\n",
    "    \"Kill yourself, nobody likes you\", \n",
    "    \"I love this community, everyone is so helpful\",\n",
    "    \"What a fucking waste of time this article is\",\n",
    "    \"The author makes some interesting points about climate change\"\n",
    "]\n",
    "\n",
    "print(\"üß™ TESTS DE PR√âDICTION BERT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    pred, prob = predict_toxicity_bert(text, model, tokenizer, device, MAX_LENGTH)\n",
    "    status = \"üî¥ TOXIC\" if pred == 1 else \"üü¢ NON-TOXIC\"\n",
    "    \n",
    "    print(f\"\\n{i}. \\\"{text}\\\"\")\n",
    "    print(f\"   ‚Üí {status} (prob: {prob:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f96b9",
   "metadata": {},
   "source": [
    "## 10. üíæ Sauvegarde des M√©tadonn√©es BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les m√©tadonn√©es BERT\n",
    "bert_metadata = {\n",
    "    'model_name': 'DistilBERT Fine-tuned',\n",
    "    'model_type': 'bert_finetuned',\n",
    "    'base_model': MODEL_NAME,\n",
    "    'f1_score': float(f1),\n",
    "    'accuracy': float(accuracy),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'auc_roc': float(auc_roc),\n",
    "    'train_time_seconds': float(training_time),\n",
    "    'inference_time_per_text_ms': float(inference_time/len(y_true)*1000),\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'training_samples': len(train_dataset),\n",
    "    'validation_samples': len(val_dataset),\n",
    "    'epochs': training_args.num_train_epochs,\n",
    "    'batch_size': training_args.per_device_train_batch_size,\n",
    "    'device': str(device),\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'criteria_met': {\n",
    "        'f1_above_075': float(f1) > 0.75,\n",
    "        'inference_below_500ms': (inference_time/len(y_true)*1000) < 500\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarder\n",
    "os.makedirs('../models/bert_model', exist_ok=True)\n",
    "with open('../models/bert_model/metadata.json', 'w') as f:\n",
    "    json.dump(bert_metadata, f, indent=2)\n",
    "\n",
    "# Sauvegarder la comparaison si disponible\n",
    "if 'comparison_df' in locals() and comparison_df is not None:\n",
    "    comparison_df.to_csv('../models/bert_model/model_comparison.csv', index=False)\n",
    "\n",
    "print(\"üíæ SAUVEGARDE TERMIN√âE\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"‚úÖ M√©tadonn√©es: ../models/bert_model/metadata.json\")\n",
    "print(f\"‚úÖ Mod√®le: ../models/bert_model/\")\n",
    "print(f\"‚úÖ Tokenizer: ../models/bert_model/\")\n",
    "\n",
    "print(f\"\\nüéâ MOD√àLE BERT TERMIN√â !\")\n",
    "print(f\"üèÜ F1-Score: {f1:.4f}\")\n",
    "print(f\"‚ö° Temps inf√©rence: {inference_time/len(y_true)*1000:.2f}ms/texte\")\n",
    "print(f\"üéØ Objectifs: F1 {'‚úÖ' if f1 > 0.75 else '‚ùå'} | Temps {'‚úÖ' if (inference_time/len(y_true)*1000) < 500 else '‚ùå'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c6330",
   "metadata": {},
   "source": [
    "## üìã R√©sum√© - Mod√®le BERT\n",
    "\n",
    "### ‚úÖ √âtapes r√©alis√©es :\n",
    "1. **Fine-tuning DistilBERT** sur donn√©es de toxicit√©\n",
    "2. **Optimisation** pour rapidit√© (DistilBERT, batch size, epochs)\n",
    "3. **√âvaluation compl√®te** avec m√©triques avanc√©es\n",
    "4. **Comparaison** avec mod√®le simple TF-IDF\n",
    "5. **Tests** sur exemples concrets\n",
    "6. **Sauvegarde** compl√®te du mod√®le\n",
    "\n",
    "### üéØ Objectifs √âTAPE 2 :\n",
    "- ‚úÖ **F1-Score > 0.75** pour le meilleur mod√®le\n",
    "- ‚úÖ **Temps d'inf√©rence < 500ms** par texte\n",
    "- ‚úÖ **Comparaison objective** et document√©e\n",
    "- ‚úÖ **Mod√®le export√©** et r√©utilisable\n",
    "\n",
    "### üìÅ Fichiers g√©n√©r√©s :\n",
    "- `models/bert_model/pytorch_model.bin` : Mod√®le BERT fine-tun√©\n",
    "- `models/bert_model/tokenizer.json` : Tokenizer DistilBERT\n",
    "- `models/bert_model/metadata.json` : M√©tadonn√©es et performances\n",
    "- `models/bert_model/model_comparison.csv` : Comparaison avec mod√®le simple\n",
    "\n",
    "### üèÜ Recommandation finale :\n",
    "**Choisir le mod√®le avec le meilleur F1-Score** pour la production, en tenant compte du trade-off performance/rapidit√© selon les besoins de l'application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

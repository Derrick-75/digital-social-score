# ğŸ—ï¸ Architecture MLOps - Digital Social Score

## ğŸ“‹ Vue d'ensemble

Cette architecture MLOps automatise l'entraÃ®nement, l'Ã©valuation et le dÃ©ploiement du modÃ¨le BERT de dÃ©tection de toxicitÃ© sur Google Cloud Platform (Vertex AI).

---

## ğŸ¯ Objectifs

1. **Automatisation complÃ¨te** : Pipeline end-to-end sans intervention manuelle
2. **ReproductibilitÃ©** : Chaque exÃ©cution traÃ§able et reproductible
3. **QualitÃ©** : DÃ©ploiement uniquement si amÃ©lioration significative (>2%)
4. **SÃ©curitÃ©** : Anonymisation RGPD avant entraÃ®nement
5. **ScalabilitÃ©** : Gestion de gros volumes de donnÃ©es

---

## ğŸ›ï¸ Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         VERTEX AI PIPELINES                              â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Ã‰TAPE 1       â”‚â”€â”€â”€â”€â–¶â”‚  Ã‰TAPE 2       â”‚â”€â”€â”€â”€â–¶â”‚  Ã‰TAPE 3        â”‚    â”‚
â”‚  â”‚  PrÃ©paration   â”‚     â”‚  EntraÃ®nement  â”‚     â”‚  Ã‰valuation &   â”‚    â”‚
â”‚  â”‚  & Anonymi-    â”‚     â”‚  BERT          â”‚     â”‚  DÃ©cision       â”‚    â”‚
â”‚  â”‚  sation        â”‚     â”‚                â”‚     â”‚                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â”‚                       â”‚                       â”‚               â”‚
â”‚         â–¼                       â–¼                       â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚              GOOGLE CLOUD STORAGE (GCS)                      â”‚     â”‚
â”‚  â”‚  - DonnÃ©es brutes      - ModÃ¨les entraÃ®nÃ©s                   â”‚     â”‚
â”‚  â”‚  - DonnÃ©es anonymisÃ©es - MÃ©triques & rapports                â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   API FASTAPI (K8s/GKE)  â”‚
                    â”‚   - InfÃ©rence temps rÃ©el â”‚
                    â”‚   - Monitoring Prometheusâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Structure des Fichiers

```
etape7-mlops/
â”‚
â”œâ”€â”€ vertex_pipelines/              # ğŸ¯ CÅ“ur du pipeline MLOps
â”‚   â”œâ”€â”€ components/                # Composants rÃ©utilisables
â”‚   â”‚   â”œâ”€â”€ prepare_data.py        # Ã‰tape 1: Anonymisation NER
â”‚   â”‚   â”œâ”€â”€ train_model.py         # Ã‰tape 2: EntraÃ®nement BERT
â”‚   â”‚   â””â”€â”€ evaluate_model.py      # Ã‰tape 3: Ã‰valuation & dÃ©cision
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline_definition.py     # DÃ©finition du pipeline complet
â”‚   â””â”€â”€ ml_pipeline_clean.json     # Pipeline compilÃ© (dÃ©ployable)
â”‚
â”œâ”€â”€ clean_and_compile.py           # ğŸ”§ Compilation du pipeline
â”œâ”€â”€ launch_pipeline_clean.py       # ğŸš€ Lancement sur Vertex AI
â”œâ”€â”€ upload_data_to_gcs.py          # ğŸ“¤ Upload donnÃ©es vers GCS
â”‚
â”œâ”€â”€ requirements.txt               # DÃ©pendances Python
â””â”€â”€ README.md                      # Documentation
```

---

## ğŸ”„ Pipeline MLOps DÃ©taillÃ©

### **Ã‰TAPE 1 : PrÃ©paration & Anonymisation des DonnÃ©es** ğŸ“Š

**Fichier :** `vertex_pipelines/components/prepare_data.py`

**ResponsabilitÃ©s :**
- âœ… TÃ©lÃ©charge les donnÃ©es brutes depuis GCS
- âœ… Charge le modÃ¨le spaCy (`en_core_web_sm`)
- âœ… Anonymise les entitÃ©s nommÃ©es (PERSON, ORG, GPE, LOC)
- âœ… Anonymise les emails (regex)
- âœ… Nettoie les donnÃ©es (supprime NaN, textes trop courts)
- âœ… Sauvegarde les donnÃ©es anonymisÃ©es
- âœ… Log les mÃ©triques (nombre d'Ã©chantillons, taux de toxicitÃ©)

**Inputs :**
- `raw_data_gcs_path` (str) : `gs://bucket/data/train.csv`

**Outputs :**
- `anonymized_data` (Dataset) : DonnÃ©es anonymisÃ©es
- `metrics` (Metrics) : num_samples, num_toxic, toxicity_rate
- `num_samples` (int) : Nombre total d'Ã©chantillons
- `num_toxic` (int) : Nombre d'Ã©chantillons toxiques

**Technologies :**
- spaCy 3.7.2 (NER)
- pandas 2.0.3
- google-cloud-storage 2.10.0

**Container :** `python:3.10-slim`

---

### **Ã‰TAPE 2 : EntraÃ®nement du ModÃ¨le BERT** ğŸ¤–

**Fichier :** `vertex_pipelines/components/train_model.py`

**ResponsabilitÃ©s :**
- âœ… Charge les donnÃ©es anonymisÃ©es
- âœ… Split train/validation (80/20, stratifiÃ©)
- âœ… Charge BERT prÃ©-entraÃ®nÃ© (`bert-base-uncased`)
- âœ… Tokenize les textes (max_length=512)
- âœ… EntraÃ®ne le modÃ¨le avec Hugging Face Trainer
- âœ… Ã‰value sur le set de validation
- âœ… Sauvegarde le meilleur modÃ¨le + tokenizer
- âœ… Log les mÃ©triques (accuracy, F1-score, loss)

**Inputs :**
- `training_data` (Dataset) : DonnÃ©es anonymisÃ©es de l'Ã©tape 1
- `epochs` (int) : Nombre d'Ã©poques (dÃ©faut: 3)
- `learning_rate` (float) : Taux d'apprentissage (dÃ©faut: 2e-5)
- `batch_size` (int) : Taille des batchs (dÃ©faut: 16)

**Outputs :**
- `model_output` (Model) : ModÃ¨le BERT entraÃ®nÃ©
- `metrics` (Metrics) : accuracy, f1_score, train_loss, eval_loss

**Technologies :**
- transformers 4.35.0 (BERT)
- torch 2.1.0
- scikit-learn 1.3.0
- accelerate 0.24.1

**Container :** `python:3.10-slim`

**HyperparamÃ¨tres :**
```python
TrainingArguments(
    num_train_epochs=2,              # Nombre d'Ã©poques
    per_device_train_batch_size=16,  # Batch size
    learning_rate=2e-5,              # Learning rate
    weight_decay=0.01,               # RÃ©gularisation L2
    eval_strategy="epoch",           # Ã‰valuation Ã  chaque Ã©poque
    save_strategy="epoch",           # Sauvegarde Ã  chaque Ã©poque
    load_best_model_at_end=True,     # Charger le meilleur modÃ¨le
    metric_for_best_model="f1",      # MÃ©trique de sÃ©lection
)
```

---

### **Ã‰TAPE 3 : Ã‰valuation & DÃ©cision de DÃ©ploiement** ğŸ¯

**Fichier :** `vertex_pipelines/components/evaluate_model.py`

**ResponsabilitÃ©s :**
- âœ… Charge les donnÃ©es de test depuis GCS
- âœ… Charge le nouveau modÃ¨le entraÃ®nÃ©
- âœ… Fait des prÃ©dictions sur le test set
- âœ… Calcule les mÃ©triques (accuracy, F1, precision, recall, confusion matrix)
- âœ… Compare avec le modÃ¨le en production (F1-score)
- âœ… **DÃ‰CISION AUTOMATIQUE** : DÃ©ployer si amÃ©lioration â‰¥ 2%
- âœ… GÃ©nÃ¨re un rapport d'Ã©valuation (JSON)
- âœ… Log toutes les mÃ©triques

**Inputs :**
- `test_data_gcs_path` (str) : `gs://bucket/data/test.csv`
- `new_model` (Model) : ModÃ¨le de l'Ã©tape 2
- `current_model_f1` (float) : F1-score du modÃ¨le actuel (dÃ©faut: 0.5)
- `improvement_threshold` (float) : Seuil d'amÃ©lioration (dÃ©faut: 0.02 = 2%)

**Outputs :**
- `metrics` (Metrics) : Toutes les mÃ©triques d'Ã©valuation
- `should_deploy` (bool) : **DÃ‰CISION** - DÃ©ployer ou non
- `new_f1_score` (float) : F1-score du nouveau modÃ¨le

**Logique de DÃ©cision :**
```python
improvement = new_f1_score - current_model_f1
should_deploy = improvement >= 0.02  # Au moins 2% d'amÃ©lioration

if should_deploy:
    print("âœ… DÃ‰PLOYER - AmÃ©lioration significative dÃ©tectÃ©e!")
else:
    print("âŒ NE PAS DÃ‰PLOYER - AmÃ©lioration insuffisante")
```

**Technologies :**
- transformers 4.35.0
- torch 2.1.0
- scikit-learn 1.3.0

**Container :** `python:3.10-slim`

---

## ğŸ—„ï¸ Organisation des DonnÃ©es (GCS)

```
gs://digitalsocialscoreapi_cloudbuild/
â”‚
â”œâ”€â”€ data/                                    # ğŸ“ DonnÃ©es sources
â”‚   â”œâ”€â”€ train.csv                            # 159,571 lignes (61.68 MB)
â”‚   â””â”€â”€ test.csv                             # 153,164 lignes (55.54 MB)
â”‚
â”œâ”€â”€ vertex-pipelines/                        # ğŸ“ Artefacts des pipelines
â”‚   â””â”€â”€ 24274638091/                         # Project ID
â”‚       â””â”€â”€ digital-social-score-ml-pipeline-YYYYMMDDHHMMSS/
â”‚           â”œâ”€â”€ prepare-data-op_*/
â”‚           â”‚   â”œâ”€â”€ anonymized_data.csv      # DonnÃ©es anonymisÃ©es
â”‚           â”‚   â””â”€â”€ metrics                  # MÃ©triques de prÃ©paration
â”‚           â”‚
â”‚           â”œâ”€â”€ train-model-op_*/
â”‚           â”‚   â”œâ”€â”€ model/                   # ModÃ¨le BERT sauvegardÃ©
â”‚           â”‚   â”‚   â”œâ”€â”€ config.json
â”‚           â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚           â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚           â”‚   â”‚   â”œâ”€â”€ vocab.txt
â”‚           â”‚   â”‚   â””â”€â”€ metadata.json        # MÃ©tadonnÃ©es d'entraÃ®nement
â”‚           â”‚   â””â”€â”€ metrics                  # MÃ©triques d'entraÃ®nement
â”‚           â”‚
â”‚           â””â”€â”€ evaluate-and-decide-op_*/
â”‚               â”œâ”€â”€ evaluation_report.json   # Rapport d'Ã©valuation complet
â”‚               â””â”€â”€ metrics                  # MÃ©triques d'Ã©valuation
â”‚
â””â”€â”€ models/                                  # ğŸ“ ModÃ¨les dÃ©ployÃ©s (si should_deploy=True)
    â””â”€â”€ production/
        â””â”€â”€ bert-toxicity-v1/                # Version actuelle en production
```

---

## âš™ï¸ Configuration Technique

### **Vertex AI Pipeline**

```python
# Projet GCP
project_id = "digitalsocialscoreapi"
project_number = "24274638091"
region = "europe-west1"

# Service Account
service_account = "24274638091-compute@developer.gserviceaccount.com"

# Machine Type
machine_type = "e2-standard-4"  # 4 vCPUs, 16 GB RAM

# Pipeline Root
pipeline_root = "gs://digitalsocialscoreapi_cloudbuild/vertex-pipelines"
```

### **ParamÃ¨tres du Pipeline**

```python
{
    "raw_data_gcs_path": "gs://digitalsocialscoreapi_cloudbuild/data/train.csv",
    "test_data_gcs_path": "gs://digitalsocialscoreapi_cloudbuild/data/test.csv",
    "epochs": 2,                    # Nombre d'Ã©poques d'entraÃ®nement
    "learning_rate": 2e-5,          # Taux d'apprentissage
    "batch_size": 16,               # Taille des batchs
    "current_model_f1": 0.5,        # F1-score du modÃ¨le actuel
    "improvement_threshold": 0.02   # Seuil d'amÃ©lioration (2%)
}
```

---

## ğŸ” SÃ©curitÃ© & ConformitÃ© RGPD

### **Anonymisation (Ã‰tape 1)**

1. **Named Entity Recognition (NER) avec spaCy**
   ```python
   # EntitÃ©s dÃ©tectÃ©es et anonymisÃ©es
   PERSON â†’ [PERSON_a3f8d2e1]
   ORG    â†’ [ORG_b7c9a4f3]
   GPE    â†’ [GPE_e2d1f8b4]
   LOC    â†’ [LOC_f9a3c7e2]
   ```

2. **Anonymisation des emails (Regex)**
   ```python
   user@example.com â†’ [EMAIL]
   ```

3. **Hash SHA-256 (8 premiers caractÃ¨res)**
   - Impossible de retrouver l'entitÃ© originale
   - Identique pour la mÃªme entitÃ© (cohÃ©rence)

### **Permissions IAM**

```yaml
Service Account: 24274638091-compute@developer.gserviceaccount.com
Roles:
  - roles/storage.objectAdmin        # Lecture/Ã©criture GCS
  - roles/aiplatform.user            # Lancement pipelines Vertex AI
  - roles/logging.logWriter          # Ã‰criture logs Cloud Logging
```

---

## ğŸ“Š MÃ©triques & Monitoring

### **MÃ©triques CollectÃ©es**

**Ã‰tape 1 - PrÃ©paration :**
- `num_samples` : Nombre total d'Ã©chantillons
- `num_toxic` : Nombre d'Ã©chantillons toxiques
- `toxicity_rate` : Taux de toxicitÃ© (%)

**Ã‰tape 2 - EntraÃ®nement :**
- `accuracy` : PrÃ©cision globale
- `f1_score` : F1-Score (mÃ©trique principale)
- `train_loss` : Loss d'entraÃ®nement
- `eval_loss` : Loss de validation

**Ã‰tape 3 - Ã‰valuation :**
- `new_accuracy` : Accuracy du nouveau modÃ¨le
- `new_f1_score` : F1-Score du nouveau modÃ¨le
- `new_precision` : PrÃ©cision
- `new_recall` : Rappel
- `improvement` : AmÃ©lioration absolue du F1
- `improvement_pct` : AmÃ©lioration en pourcentage
- `should_deploy` : DÃ©cision de dÃ©ploiement (0/1)

### **Visualisation**

- **Vertex AI Console** : Graphiques de mÃ©triques intÃ©grÃ©s
- **Cloud Logging** : Logs dÃ©taillÃ©s de chaque Ã©tape
- **Artifact Registry** : Versioning des modÃ¨les

---

## ğŸš€ Workflows de DÃ©ploiement

### **1. DÃ©ploiement Initial**

```bash
# 1. PrÃ©parer les donnÃ©es
python upload_data_to_gcs.py

# 2. Compiler le pipeline
python clean_and_compile.py

# 3. Lancer le pipeline
python launch_pipeline_clean.py
```

### **2. RÃ©entraÃ®nement Automatique (Scheduled)**

```yaml
# Cloud Scheduler (Ã  configurer)
FrÃ©quence: Hebdomadaire (chaque lundi Ã  2h00)
Trigger: Cloud Function â†’ launch_pipeline_clean.py
DonnÃ©es: Nouvelles donnÃ©es chargÃ©es automatiquement dans GCS
```

### **3. CI/CD avec Git**

```yaml
# .github/workflows/mlops-pipeline.yml
on:
  push:
    branches: [main]
    paths: ['etape7-mlops/**']

jobs:
  deploy-pipeline:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Authenticate GCP
        uses: google-github-actions/auth@v1
      - name: Compile Pipeline
        run: python clean_and_compile.py
      - name: Launch Pipeline
        run: python launch_pipeline_clean.py
```

---

## ğŸ”„ DÃ©pendances entre Composants

```mermaid
graph LR
    A[prepare-data-op] -->|anonymized_data| B[train-model-op]
    B -->|model_output| C[evaluate-and-decide-op]
    D[raw_data_gcs_path] --> A
    E[test_data_gcs_path] --> C
    C -->|should_deploy=True| F[DÃ©ploiement Production]
    C -->|should_deploy=False| G[Archivage ModÃ¨le]
```

---

## ğŸ“¦ Versions & DÃ©pendances

### **Python**
- **Local** : Python 3.13.7
- **Containers** : Python 3.10-slim

### **Packages ClÃ©s**

```txt
# Pipeline Orchestration
kfp==2.14.6                      # Kubeflow Pipelines SDK
google-cloud-aiplatform==1.126.1 # Vertex AI SDK

# Machine Learning
transformers==4.35.0             # BERT
torch==2.1.0                     # PyTorch
scikit-learn==1.3.0              # MÃ©triques

# Data Processing
pandas==2.0.3                    # DataFrames
spacy==3.7.2                     # NER

# Cloud
google-cloud-storage==2.10.0     # GCS
```

---

## ğŸ¯ MÃ©triques de Performance

### **Objectifs**

| MÃ©trique | Objectif | Actuel |
|----------|----------|--------|
| **F1-Score** | â‰¥ 0.85 | ğŸ¯ Ã€ mesurer |
| **Accuracy** | â‰¥ 0.90 | ğŸ¯ Ã€ mesurer |
| **Precision** | â‰¥ 0.85 | ğŸ¯ Ã€ mesurer |
| **Recall** | â‰¥ 0.80 | ğŸ¯ Ã€ mesurer |

### **SLA Pipeline**

| Phase | DurÃ©e EstimÃ©e | Machine |
|-------|---------------|---------|
| PrÃ©paration (159k samples) | 5-10 min | e2-standard-4 |
| EntraÃ®nement (2 epochs) | 30-45 min | e2-standard-4 |
| Ã‰valuation (153k samples) | 5-10 min | e2-standard-4 |
| **TOTAL** | **40-65 min** | - |

---

## ğŸ› Troubleshooting

### **ProblÃ¨mes Courants**

1. **Pipeline failed: prepare-data-op**
   - âŒ Cause: spaCy model download failed
   - âœ… Solution: VÃ©rifier connexion internet container, augmenter timeout

2. **Out of Memory (OOM)**
   - âŒ Cause: Batch size trop grand, donnÃ©es trop volumineuses
   - âœ… Solution: RÃ©duire `batch_size` de 16 â†’ 8, augmenter machine type

3. **GCS Permission Denied**
   - âŒ Cause: Service account sans droits storage
   - âœ… Solution: `gcloud projects add-iam-policy-binding`

4. **Unicode Encoding Errors**
   - âŒ Cause: Emojis/accents dans les fichiers Python
   - âœ… Solution: `clean_and_compile.py` (dÃ©jÃ  implÃ©mentÃ©)

---

## ğŸ“š Ressources ComplÃ©mentaires

- [Vertex AI Pipelines Documentation](https://cloud.google.com/vertex-ai/docs/pipelines)
- [Kubeflow Pipelines SDK](https://kubeflow-pipelines.readthedocs.io/)
- [BERT Fine-tuning Guide](https://huggingface.co/docs/transformers/training)
- [spaCy NER](https://spacy.io/usage/linguistic-features#named-entities)

---

## âœ… Checklist de Production

- [x] Pipeline compilÃ© et testÃ©
- [x] DonnÃ©es uploadÃ©es sur GCS
- [x] Anonymisation RGPD conforme
- [x] Service account configurÃ©
- [ ] Pipeline exÃ©cutÃ© avec succÃ¨s (en cours)
- [ ] ModÃ¨le dÃ©ployÃ© en production
- [ ] Monitoring Prometheus actif
- [ ] CI/CD configurÃ©
- [ ] Alertes configurÃ©es

---

**DerniÃ¨re mise Ã  jour :** 9 novembre 2025
**Auteur :** Digital Social Score Team
**Version :** 1.0.0

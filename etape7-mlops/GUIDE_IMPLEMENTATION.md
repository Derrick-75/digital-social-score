# ğŸ¯ Guide d'ImplÃ©mentation MLOps - Digital Social Score

## ğŸ“Œ Vue SynthÃ©tique

Cette architecture MLOps permet l'**entraÃ®nement automatique et le dÃ©ploiement conditionnel** du modÃ¨le BERT de dÃ©tection de toxicitÃ©.

---

## ğŸ—ï¸ Composants Principaux

### 1ï¸âƒ£ **Vertex AI Pipelines** (Orchestration)
- **RÃ´le** : Orchestrer les 3 Ã©tapes du pipeline MLOps
- **Technologie** : Kubeflow Pipelines (KFP 2.14.6)
- **HÃ©bergement** : Vertex AI (GCP)

### 2ï¸âƒ£ **Cloud Storage** (Stockage)
- **RÃ´le** : Stocker donnÃ©es, modÃ¨les, artefacts
- **Bucket** : `gs://digitalsocialscoreapi_cloudbuild/`
- **DurabilitÃ©** : 99.999999999% (11 nines)

### 3ï¸âƒ£ **Container Registry** (Images)
- **RÃ´le** : Images Docker des composants pipeline
- **Base Image** : `python:3.10-slim`
- **Auto-build** : Oui (par Vertex AI)

### 4ï¸âƒ£ **API FastAPI** (Production)
- **RÃ´le** : Servir les prÃ©dictions en temps rÃ©el
- **HÃ©bergement** : GKE (Kubernetes)
- **ScalabilitÃ©** : Auto-scaling horizontal

### 5ï¸âƒ£ **Monitoring** (ObservabilitÃ©)
- **Prometheus** : MÃ©triques applicatives
- **Cloud Logging** : Logs centralisÃ©s
- **Vertex AI Metadata** : Tracking des exÃ©cutions

---

## ğŸ”„ Flux de DonnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DonnÃ©es Brutes â”‚  train.csv (159k lignes)
â”‚   (GCS)         â”‚  test.csv (153k lignes)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PIPELINE VERTEX AI (ExÃ©cution ~45-60 min)              â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Ã‰TAPE 1: prepare_data_op (5-10 min)      â”‚          â”‚
â”‚  â”‚ â€¢ TÃ©lÃ©charge train.csv depuis GCS        â”‚          â”‚
â”‚  â”‚ â€¢ Charge modÃ¨le spaCy (en_core_web_sm)   â”‚          â”‚
â”‚  â”‚ â€¢ DÃ©tecte entitÃ©s (PERSON, ORG, GPE...)  â”‚          â”‚
â”‚  â”‚ â€¢ Anonymise avec hash SHA-256            â”‚          â”‚
â”‚  â”‚ â€¢ Nettoie donnÃ©es (remove NaN, courts)   â”‚          â”‚
â”‚  â”‚ OUTPUT: anonymized_data.csv              â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                 â”‚                                        â”‚
â”‚                 â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Ã‰TAPE 2: train_model_op (30-45 min)      â”‚          â”‚
â”‚  â”‚ â€¢ Charge anonymized_data.csv             â”‚          â”‚
â”‚  â”‚ â€¢ Split train/val (80/20 stratifiÃ©)      â”‚          â”‚
â”‚  â”‚ â€¢ Tokenize avec BERT tokenizer           â”‚          â”‚
â”‚  â”‚ â€¢ Fine-tune bert-base-uncased (2 epochs) â”‚          â”‚
â”‚  â”‚ â€¢ Ã‰value sur validation set              â”‚          â”‚
â”‚  â”‚ OUTPUT: model_output/ (BERT + tokenizer) â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                 â”‚                                        â”‚
â”‚                 â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Ã‰TAPE 3: evaluate_and_decide_op (5-10m)  â”‚          â”‚
â”‚  â”‚ â€¢ Charge test.csv depuis GCS             â”‚          â”‚
â”‚  â”‚ â€¢ Charge model_output de l'Ã©tape 2       â”‚          â”‚
â”‚  â”‚ â€¢ PrÃ©dit sur test set (153k samples)     â”‚          â”‚
â”‚  â”‚ â€¢ Calcule F1, Accuracy, Precision, Recallâ”‚          â”‚
â”‚  â”‚ â€¢ Compare F1 avec modÃ¨le actuel          â”‚          â”‚
â”‚  â”‚ DÃ‰CISION:                                 â”‚          â”‚
â”‚  â”‚   âœ… if F1_new - F1_current >= 0.02:     â”‚          â”‚
â”‚  â”‚      â†’ should_deploy = True              â”‚          â”‚
â”‚  â”‚   âŒ else:                                â”‚          â”‚
â”‚  â”‚      â†’ should_deploy = False             â”‚          â”‚
â”‚  â”‚ OUTPUT: should_deploy, new_f1_score      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                 â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ should_deploy? â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
              â”‚       â”‚
        YES â”€â”€â”˜       â””â”€â”€ NO
         â”‚                 â”‚
         â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DÃ©ploiement API â”‚  â”‚  Archivage   â”‚
â”‚  (Production)   â”‚  â”‚   ModÃ¨le     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API FastAPI    â”‚
â”‚  â€¢ GET /health  â”‚
â”‚  â€¢ POST /predictâ”‚
â”‚  â€¢ GET /metrics â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Utilisateurs
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š DonnÃ©es et Formats

### **DonnÃ©es d'EntrÃ©e** (GCS)

**train.csv** (159,571 lignes, 61.68 MB)
```csv
comment_text,toxic
"This is a normal comment",0
"You are stupid idiot!",1
...
```

**test.csv** (153,164 lignes, 55.54 MB)
```csv
comment_text,toxic
"Another comment here",0
"Offensive content...",1
...
```

### **DonnÃ©es AnonymisÃ©es** (AprÃ¨s Ã‰tape 1)

```csv
text_anonymized,toxic
"This is a normal comment",0
"You are [PERSON_a3f8d2e1] idiot!",1
"[ORG_b7c9a4f3] is located in [GPE_e2d1f8b4]",0
"Contact me at [EMAIL]",0
...
```

### **ModÃ¨le SauvegardÃ©** (AprÃ¨s Ã‰tape 2)

```
model_output/
â”œâ”€â”€ config.json              # Configuration BERT
â”œâ”€â”€ pytorch_model.bin        # Poids du modÃ¨le (420 MB)
â”œâ”€â”€ tokenizer_config.json    # Config tokenizer
â”œâ”€â”€ vocab.txt                # Vocabulaire
â”œâ”€â”€ special_tokens_map.json  # Tokens spÃ©ciaux
â””â”€â”€ metadata.json            # MÃ©tadonnÃ©es custom
    {
      "model_name": "bert-base-uncased",
      "epochs": 2,
      "learning_rate": 2e-05,
      "batch_size": 16,
      "accuracy": 0.9234,
      "f1_score": 0.8567,
      "num_train_samples": 127656,
      "num_val_samples": 31914
    }
```

### **Rapport d'Ã‰valuation** (AprÃ¨s Ã‰tape 3)

```json
{
  "accuracy": 0.9301,
  "f1_score": 0.8723,
  "precision": 0.8912,
  "recall": 0.8541,
  "current_f1": 0.8500,
  "improvement": 0.0223,
  "improvement_pct": 2.62,
  "should_deploy": true,
  "confusion_matrix": [[135201, 2341], [1832, 13790]],
  "classification_report": "..."
}
```

---

## âš™ï¸ Configuration & ParamÃ¨tres

### **Variables d'Environnement**

```bash
# GCP Configuration
export PROJECT_ID="digitalsocialscoreapi"
export PROJECT_NUMBER="24274638091"
export REGION="europe-west1"
export SERVICE_ACCOUNT="24274638091-compute@developer.gserviceaccount.com"

# GCS Paths
export BUCKET="gs://digitalsocialscoreapi_cloudbuild"
export TRAIN_DATA="${BUCKET}/data/train.csv"
export TEST_DATA="${BUCKET}/data/test.csv"
export PIPELINE_ROOT="${BUCKET}/vertex-pipelines"

# Pipeline Parameters
export EPOCHS=2
export LEARNING_RATE=0.00002
export BATCH_SIZE=16
export CURRENT_MODEL_F1=0.50
export IMPROVEMENT_THRESHOLD=0.02
```

### **ParamÃ¨tres Modifiables**

| ParamÃ¨tre | Valeur Actuelle | Effet si AugmentÃ© | Effet si DiminuÃ© |
|-----------|-----------------|-------------------|------------------|
| `epochs` | 2 | â¬†ï¸ Meilleure performance (mais risque overfitting) | â¬‡ï¸ Plus rapide, moins bon |
| `learning_rate` | 2e-5 | âš ï¸ Convergence instable | ğŸŒ Convergence lente |
| `batch_size` | 16 | ğŸš€ Plus rapide (mais plus de RAM) | ğŸ’¾ Moins de RAM, plus lent |
| `improvement_threshold` | 0.02 (2%) | ğŸ”’ DÃ©ploiement plus strict | âš¡ DÃ©ploiement plus frÃ©quent |

---

## ğŸ” SÃ©curitÃ© & ConformitÃ©

### **Anonymisation RGPD**

1. **EntitÃ©s DÃ©tectÃ©es**
   - `PERSON` : Noms de personnes â†’ `[PERSON_hash]`
   - `ORG` : Organisations â†’ `[ORG_hash]`
   - `GPE` : EntitÃ©s gÃ©opolitiques â†’ `[GPE_hash]`
   - `LOC` : Lieux â†’ `[LOC_hash]`

2. **Emails**
   - DÃ©tection : Regex `\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b`
   - Remplacement : `[EMAIL]`

3. **Hash SHA-256**
   - Algorithme : SHA-256
   - Taille : 8 premiers caractÃ¨res
   - Exemple : `John Doe` â†’ `a3f8d2e1`

### **Permissions IAM**

```yaml
Service Account: 24274638091-compute@developer.gserviceaccount.com

RÃ´les Requis:
  âœ… roles/storage.objectAdmin          # Lecture/Ã©criture GCS
  âœ… roles/aiplatform.user              # Lancer pipelines Vertex AI
  âœ… roles/logging.logWriter            # Logs Cloud Logging
  âœ… roles/artifactregistry.reader      # Lire images containers
```

---

## ğŸš€ Guide de DÃ©ploiement

### **1. Setup Initial (Une seule fois)**

```bash
# 1. Installer dÃ©pendances locales
cd etape7-mlops
pip install -r requirements.txt

# 2. Authentification GCP
gcloud auth login
gcloud config set project digitalsocialscoreapi

# 3. Uploader les donnÃ©es vers GCS
python upload_data_to_gcs.py
# âœ… train.csv â†’ gs://digitalsocialscoreapi_cloudbuild/data/train.csv
# âœ… test.csv  â†’ gs://digitalsocialscoreapi_cloudbuild/data/test.csv
```

### **2. Lancement du Pipeline**

```bash
# 1. Compiler le pipeline (gÃ©nÃ¨re ml_pipeline_clean.json)
python clean_and_compile.py
# âœ… Pipeline compilÃ©: vertex_pipelines/ml_pipeline_clean.json (26 KB)

# 2. Lancer sur Vertex AI (depuis Cloud Shell ou avec ADC)
python launch_pipeline_clean.py
# âœ… Pipeline job crÃ©Ã©: digital-social-score-ml-pipeline-YYYYMMDDHHMMSS
```

### **3. Monitoring de l'ExÃ©cution**

```bash
# Option 1: Console Web
https://console.cloud.google.com/vertex-ai/pipelines/runs?project=digitalsocialscoreapi

# Option 2: CLI
gcloud ai pipeline-jobs list \
  --region=europe-west1 \
  --project=digitalsocialscoreapi

# Option 3: Logs
gcloud logging read \
  "resource.type=ml_job" \
  --limit=50 \
  --format=json
```

### **4. VÃ©rification des RÃ©sultats**

```bash
# Lister les artefacts gÃ©nÃ©rÃ©s
gsutil ls -lhr gs://digitalsocialscoreapi_cloudbuild/vertex-pipelines/

# TÃ©lÃ©charger le rapport d'Ã©valuation
gsutil cp gs://digitalsocialscoreapi_cloudbuild/vertex-pipelines/.../evaluation_report.json .

# Voir la dÃ©cision de dÃ©ploiement
cat evaluation_report.json | jq '.should_deploy'
```

---

## ğŸ“ˆ MÃ©triques de SuccÃ¨s

### **MÃ©triques Techniques**

| MÃ©trique | Objectif | Importance |
|----------|----------|------------|
| **F1-Score** | â‰¥ 0.85 | ğŸ”´ Critique (dÃ©cision de dÃ©ploiement) |
| **Accuracy** | â‰¥ 0.90 | ğŸŸ¡ Important |
| **Precision** | â‰¥ 0.85 | ğŸŸ¢ Souhaitable (minimiser faux positifs) |
| **Recall** | â‰¥ 0.80 | ğŸŸ¢ Souhaitable (minimiser faux nÃ©gatifs) |

### **MÃ©triques OpÃ©rationnelles**

| MÃ©trique | Objectif | Mesure |
|----------|----------|--------|
| **Pipeline Success Rate** | â‰¥ 95% | % exÃ©cutions sans erreur |
| **Pipeline Duration** | â‰¤ 60 min | Temps total d'exÃ©cution |
| **Cost per Run** | â‰¤ $5 | CoÃ»t GCP par exÃ©cution |
| **Model Improvement Rate** | â‰¥ 30% | % de runs qui dÃ©ploient |

---

## ğŸ› Debugging & Troubleshooting

### **ProblÃ¨me 1: Pipeline Failed - prepare-data-op**

**SymptÃ´mes:**
```
Error: The replica workerpool0-0 exited with a non-zero status of 1
```

**Causes Possibles:**
1. âŒ spaCy model download Ã©choue (connexion internet)
2. âŒ DonnÃ©es GCS inaccessibles (permissions)
3. âŒ Out of Memory (dataset trop gros)

**Solutions:**
```python
# 1. VÃ©rifier que spaCy tÃ©lÃ©charge bien le modÃ¨le
os.system("python -m spacy download en_core_web_sm")
# â†’ Regarder les logs pour "âœ” Download and installation successful"

# 2. VÃ©rifier permissions GCS
gsutil ls gs://digitalsocialscoreapi_cloudbuild/data/
# â†’ Doit lister train.csv et test.csv

# 3. Augmenter timeout ou rÃ©duire dataset
# Dans @component decorator:
@component(
    base_image="python:3.10-slim",
    timeout="3600s"  # 1 heure au lieu de 20 min
)
```

### **ProblÃ¨me 2: Out of Memory (OOM)**

**SymptÃ´mes:**
```
Error: Container killed due to memory limit exceeded
```

**Solutions:**
```python
# 1. RÃ©duire batch_size
batch_size = 8  # Au lieu de 16

# 2. Augmenter machine type (dans launch_pipeline_clean.py)
# Changer de e2-standard-4 (16 GB) Ã  e2-standard-8 (32 GB)

# 3. Traiter les donnÃ©es par chunks
for chunk in pd.read_csv('/tmp/raw_data.csv', chunksize=10000):
    process_chunk(chunk)
```

### **ProblÃ¨me 3: Unicode Encoding Errors**

**SymptÃ´mes:**
```
UnicodeEncodeError: 'ascii' codec can't encode character
```

**Solution:**
```bash
# clean_and_compile.py gÃ¨re dÃ©jÃ  Ã§a automatiquement
python clean_and_compile.py
# â†’ Supprime tous les emojis et accents
```

---

## ğŸ“š Ressources & Documentation

### **Documentation Officielle**
- [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines)
- [Kubeflow Pipelines SDK](https://kubeflow-pipelines.readthedocs.io/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [spaCy NER](https://spacy.io/usage/linguistic-features#named-entities)

### **Exemples & Tutoriels**
- [Vertex AI Pipeline Samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples)
- [BERT Fine-tuning Tutorial](https://huggingface.co/docs/transformers/training)

### **Support**
- **Issues GitHub** : [Derrick-75/digital-social-score/issues](https://github.com/Derrick-75/digital-social-score/issues)
- **Documentation Interne** : `etape7-mlops/README.md`

---

## âœ… Checklist de Production

### **Avant le Premier Lancement**
- [x] DonnÃ©es uploadÃ©es sur GCS (train.csv, test.csv)
- [x] Service account configurÃ© avec bonnes permissions
- [x] Pipeline compilÃ© (`ml_pipeline_clean.json`)
- [ ] Tests unitaires des composants passÃ©s
- [ ] ExÃ©cution pipeline de bout en bout rÃ©ussie

### **Avant la Mise en Production**
- [ ] ModÃ¨le dÃ©ployÃ© avec F1 â‰¥ 0.85
- [ ] API FastAPI dÃ©ployÃ©e sur GKE
- [ ] Monitoring Prometheus configurÃ©
- [ ] Alertes CloudWatch/Prometheus configurÃ©es
- [ ] Documentation Ã  jour
- [ ] Runbook d'incident crÃ©Ã©

### **Maintenance Continue**
- [ ] RÃ©entraÃ®nement hebdomadaire automatisÃ© (Cloud Scheduler)
- [ ] Dashboard de monitoring crÃ©Ã© (Grafana)
- [ ] Logs centralisÃ©s (Cloud Logging)
- [ ] Backups GCS configurÃ©s
- [ ] Plan de disaster recovery testÃ©

---

**Version:** 1.0.0  
**Date:** 9 novembre 2025  
**Auteurs:** Digital Social Score Team  
**Contact:** [GitHub Issues](https://github.com/Derrick-75/digital-social-score/issues)
